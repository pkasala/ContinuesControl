# Continues Control - Moving Arm - Agent Learning process
### Introduction
The aim of this article is to describe how the agent learns. 
This is actually additional information to Report.pdf, which describes the benefit of TD3 algorithm - which is based on Actor-Critic model.

The full learning process is based on off-policy method. This means learning process is not directly on each action, but instead using some kind of experience memory.
Each experience is representing by tuple State, Action, Reward, NextState.
The agent starts with getting the experiences by execution fully random actions. (Improvement called local exploration is excused from "fully random action", this topic describe later in article).
This phase is called warm-up and helps to randomize the action space.
Each agent experience is than stored in some memory. 

After warm-up finish the agent starts to learn from Actor model.
First takes the random mini-batch of experiences from memory - this step is called experience sampling.
From each experience perform the TD3 algorithm steps (describe later).
After certain learning steps, the current knowleadge is moved to target network copy to stabilize the lerning process. 

TD3 Algorithm:
The algorithm is based on Actor-Critic algorithm with target Neural Network.This means each Neural Network has a target copy. And during the process the knowleadge of local neural newtork is copied (weight and bias values of neurons are copied over) to target network.
Basic Actor-Critic contains two NeuralNetwork. Where Actor NN (Policy NN) learns the optimal action for each state and Critic NN learns the Q-Value for state+action. This learning process happend in same time, which is not an optimal solution, and lead to the problem with aproximation error. 
TD3 algorithm extend the basic Actor-Critic model by adding second Critic model. 
Steps:
First - from experience and the NextState from it, the Target Actor NN learns the NextAction
Second - than both Target Critic NN learns the Next Q-Values based on NextState+NextAction
Third - take the minimum of Next Q-Values, which is actually the 
